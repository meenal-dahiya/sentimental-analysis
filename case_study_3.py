# -*- coding: utf-8 -*-
"""case study 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-4UWHTADVg4U_uh4CsvY8eVPt5ihIpUv
"""

import pandas as pd
import numpy as np
pd.set_option('max_colwidth', None)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist

!pip install -u sentence-transformers -q

from google.colab import drive
drive.mount('/content/drive')

reviews=pd.read_csv("/content/news_articles.csv")

data = reviews.copy()

data.loc[1,'Text']

data.head()

data.loc[3, 'Text']

data.tail

data.shape

data.isnull().sum()

data.duplicated().sum()

data=data.drop_duplicates()
data.reset_index(drop=True, inplace=True)

data.duplicated().sum()

data.shape

#Defining the model
#Model Building
#question - why we are usng this transformer why it is sutiable for this and use another transformer
#We'll be using the **all-MiniLM-L6-v2** model here.

# The **all-MiniLM-L6-v2** model is an all-round (**all**) model trained on a large and diverse dataset of over 1 billion training samples and generates state-of-the-art sentence embeddings of 384 dimensions.

# It is a language model (**LM**) that has 6 transformer encoder layers (**L6**) and is a smaller model (**Mini**) trained to mimic the performance of a larger model (BERT).

# Potential use-cases include text classification, sentiment analysis, and semantic search.

""" hf_xet is a helper package for enhancing file transfers with the Hugging Face Hub.
It integrates Rust-based code for efficient, chunk-based deduplication,
and caching when uploading or downloading large files"""
!pip install hf_xet

from sentence_transformers import SentenceTransformer
#Defining the model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

#Encoding the dataset

embedding_matrix = model.encode(data['Text'], show_progress_bar=True)
embedding_matrix.shape#here we are using dynamic embeddings, we do embedding in word or sentence,when we have to use sentence embedding or when we have to use word ebedding?

# printing the shape of the embedding matrix
embedding_matrix.shape

#Each news article has been converted to a 384-dimensional vector

# printing the embedding vector of the first review in the dataset
len(embedding_matrix[0])

a= "Football (soccer) is one of the most popular sports worldwide, played by millions of athletes across all continents. The game involves two teams trying to score goals by moving the ball into the opponent’s net while following specific rules. Major tournaments like the FIFA World Cup attract global attention, showcasing the skill, teamwork, and strategy that make football a universal language of competition and passion."
b= "Cricket is a bat-and-ball game widely followed in countries like India, England, and Australia. Two teams compete to score runs, with one side batting and the other bowling and fielding. Test matches, One-Day Internationals, and T20 games offer different formats, making cricket versatile and appealing to various audiences. Legendary players and intense rivalries contribute to the sport’s rich history and excitement."
c= "Artificial intelligence is transforming industries by enabling machines to perform tasks that traditionally required human intelligence. From natural language processing to autonomous vehicles, AI systems rely on vast amounts of data to learn and make predictions. This rapid advancement raises ethical questions but also presents opportunities for innovation and improved quality of life across sectors like healthcare, finance, and education."

# defining a function to compute the cosine similarity between two embedding vectors
def cosine_score(text1,text2):
    # encoding the text
    embeddings1 = model.encode(text1)
    embeddings2 = model.encode(text2)

    # calculating the L2 norm of the embedding vector
    norm1 = np.linalg.norm(embeddings1)
    norm2 = np.linalg.norm(embeddings2)

    # computing the cosine similarity
    cosine_similarity_score = ((np.dot(embeddings1,embeddings2))/(norm1*norm2))

    return cosine_similarity_score

print(cosine_score(a,b))
print(cosine_score(b,c))
print(cosine_score(a,c))

# ussing prebuilt method
from sentence_transformers import util

embeddings1 = model.encode(a)
embeddings2 = model.encode(b)
embeddings3 = model.encode(c)

print(util.cos_sim(embeddings1, embeddings2))
print(util.cos_sim(embeddings2, embeddings3))
print(util.cos_sim(embeddings1, embeddings3))

# defining a function to find the top k similar sentences for a given query
def top_k_similar_sentences(embedding_matrix, query_text, k):
    # encoding the query text
    query_embedding = model.encode(query_text)

    # calculating the cosine similarity between the query vector and all other encoded vectors of our dataset
    score_vector = np.dot(embedding_matrix,query_embedding)

    # sorting the scores in descending order and choosing the first k
    top_k_indices = np.argsort(score_vector)[::-1][:k]

    # returning the corresponding reviews
    return data.loc[list(top_k_indices), 'Text']

# defining the query text
query_text = "Budget for elections"

# displaying the top 3 similar sentences
top_k_reviews = top_k_similar_sentences(embedding_matrix, query_text, 3)

for i in top_k_reviews:
    print(i, end="\n")
    print("*******************************************************************")
    print("\n")

#clustring
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist, pdist
from sklearn.metrics import silhouette_score

meanDistortions = []
clusters = range(2, 11)

for k in clusters:
    clusterer = KMeans(n_clusters=k, random_state=1)
    clusterer.fit(embedding_matrix)

    prediction = clusterer.predict(embedding_matrix)

    distortion = sum(
        np.min(cdist(embedding_matrix, clusterer.cluster_centers_, "euclidean"), axis=1) ** 2
    )
    meanDistortions.append(distortion)

    print("Number of Clusters:", k, "\tAverage Distortion:", distortion)

plt.plot(clusters, meanDistortions, "bx-")
plt.xlabel("k")
plt.ylabel("Average Distortion")
plt.title("Selecting k with the Elbow Method", fontsize=20)
plt.show()

#silhoute score

sil_score = []
cluster_list = range(2, 10)

for n_clusters in cluster_list:
    clusterer = KMeans(n_clusters=n_clusters, random_state=1)

    preds = clusterer.fit_predict((embedding_matrix))

    score = silhouette_score(embedding_matrix, preds)
    sil_score.append(score)

    print("For n_clusters = {}, the silhouette score is {})".format(n_clusters, score))

plt.plot(cluster_list, sil_score, "bx-")
plt.show()

#why we are taking 6 encoder and decoder?
#because it give best result it depend on how valid and correct result it give it is all about hit and trail
#how we evaluate any llm model?
#multi modal model research is going on
#how do transformers learn context from the data?
#by using attention mechanism
#issue in rnn and lstm->cannot remember long sequences and by using trnsfoermer this problem is solved
#to know sequence of the tokens that why we use postional encoding We add positional information to each token’s embedding before feeding it into the encoder.
#This gives the model a way to distinguish “first word”, “second word”, etc.
#Transformers use self-attention, which treats all tokens in a sequence at once.Unlike RNNs (which read left-to-right) or CNNs (which have local kernels),attention itself has no built-in notion of order.
#transformer,llm,rag,langchain

# defining the number of clusters/categories
n_categories = 5

# fitting the model
kmeans = KMeans(n_clusters=n_categories, random_state=1).fit(embedding_matrix)

# checking the cluster centers
centers = kmeans.cluster_centers_
centers

# creating a copy of the data
clustered_data = data.copy()

# assigning the cluster/category labels
clustered_data['Category'] = kmeans.labels_

clustered_data.head()

# for each cluster, printing the 5 random news articles
for i in range(5):
    print("CLUSTER",i)
    print(clustered_data.loc[clustered_data.Category == i, 'Text'].sample(5, random_state=1).values)
    print("*****************************************************************")
    print("\n")

# dictionary of cluster label to category
category_dict = {
    0: 'Sports',
    1: 'Politics',
    2: 'Entertainment',
    3: 'Business',
    4: 'Technology'
}
# mapping cluster labels to categories
clustered_data['Category'] = clustered_data['Category'].map(category_dict)

clustered_data.head()

# loading the actual labels
labels = pd.read_excel("/content/news_article_labels.xlsx")

labels.shape

# checking the unique labels
labels['Label'].unique()

# adding the actual categories to our dataframe
clustered_data['Actual Category'] = labels['Label'].values

from sklearn.metrics import classification_report

print(classification_report(clustered_data['Actual Category'], clustered_data['Category']))

# creating a dataframe of incorrect categorizations
incorrect_category_data = clustered_data[clustered_data['Actual Category'] != clustered_data['Category']].copy()
incorrect_category_data.shape

incorrect_category_data.head()

idx = 24

print('Distance from Actual Category')
print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[2]], "euclidean")[0,0])

print('Distance from Predicted Category')
print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[3]], "euclidean")[0,0])

idx = 45

print('Distance from Actual Category')
print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[2]], "euclidean")[0,0])

print('Distance from Predicted Category')
print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[4]], "euclidean")[0,0])







