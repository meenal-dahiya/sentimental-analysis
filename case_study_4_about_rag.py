# -*- coding: utf-8 -*-
"""case study 4 about rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mh_jMTpwYM5l_so_gG6dMS9g24qkU3Fn
"""

# Retrieval-Augmented Generation
# It’s a technique used in large language models where the model retrieves relevant information from an external knowledge base (vector database, search index, documents, etc.) and then augments its generated answer with that retrieved context.
# This approach helps models:
# Reduce hallucinations
#Provide more factual / up-to-date answers
#  Handle domain-specific or long-tail knowledge
# chunking,embedding,vector database(eg-faiss) explain these search

!pip install langchain_community openai tiktoken rapidocr-onnxruntime

from google.colab import userdata
userdata.get('Gemini_API_Key')

from google.colab import userdata
GEMINI_API_KEY = userdata.get('Gemini_API_Key')

import os
os.environ["Gemini_API_Key"] = GEMINI_API_KEY

#Three stages of RAG
#Data Injestion
#Data Retrieval
#Data Generation

#Data Injestion

import requests # Module to load the data
from langchain.document_loaders import TextLoader # Module to load the data
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
from langchain.vectorstores import FAISS

!pip install unstructured

loader  = UnstructuredWordDocumentLoader('/content/Case Study 3 (1).docx')

!pip install python-docx

document = loader.load()

print(document[0].page_content)

#Chunking of Data

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

text_chunks = text_splitter.split_documents(documents=document)

len(text_chunks)

print(text_chunks[0].page_content)

#Convert chunks into Embedding (before saving the data)

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

!pip install faiss-cpu

!pip install langchain-google-genai

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.vectorstores import FAISS
from google.colab import userdata
import os

GEMINI_API_KEY = userdata.get('Gemini_API_Key')
os.environ["GOOGLE_API_KEY"] = GEMINI_API_KEY # Explicitly set the environment variable


embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GEMINI_API_KEY)



pip install sentence-transformers langchain-community faiss-cpu

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(text_chunks, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})  # top 3 relevant chunks

vector_store

retriever = vector_store.as_retriever()

from langchain.prompts import ChatPromptTemplate

template = """You are a helpful AI assistant.
Use the following context to answer the question.
If you don't know the answer, say "I don’t know" — don’t make up anything.
Keep your answer concise (max 5 sentences).

Question: {question}
Context: {context}
Answer:
"""

prompt = ChatPromptTemplate.from_template(template=template)

from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

output_parser = StrOutputParser()

pip uninstall -y langchain-google-genai google-generativeai langchain

pip install --upgrade langchain langchain-google-genai google-generativeai faiss-cpu

from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

llm_model = ChatGoogleGenerativeAI(
    model="models/gemini-1.5-flash",
    google_api_key=GEMINI_API_KEY,
    temperature=0.2
)

from langchain.chains import RetrievalQA

!pip uninstall openai -y

import os
os.environ["GOOGLE_API_KEY"] = "AIzaSyBzkKnXI0flkhIcazzMYZPhlf2CnYtpaRM"

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.runnable import RunnableMap

rag_chain = (
    RunnableMap({
        "context": retriever,
        "question": RunnablePassthrough()
    })
    | prompt
    | llm_model
    | StrOutputParser()
)

# This is Langchain Expression Language (not chains concept in Langchain)
# rag_chain = (
    # {'context':retriever, 'question':RunnablePassthrough()}
    # | prompt
    # | llm_model
    # | output_parser
# )
#rag_chain = RetrievalQA.from_chain_type(llm=llm_model, retriever=vector_store.as_retriever())

query = "How AI can be harmful to humans?"
response = rag_chain.invoke(query)





# Installation for GPU llama-cpp-python
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28  --force-reinstall --upgrade --no-cache-dir -q 2>/dev/null

!pip install tiktoken pypdf langchain langchain-community chromadb sentence-transformers huggingface_hub

import json
import tiktoken
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
from google.colab import userdata, drive

apple_pdf_path = '/content/HBR_How_Apple_Is_Organized_For_Innovation-4 (2).pdf'

from google.colab import drive
drive.mount('/content/HBR')

from langchain_community.document_loaders import PyMuPDFLoader

pdf_loader = PyMuPDFLoader(apple_pdf_path)

!pip install pymupdf

apple = pdf_loader.load()

for i in range(3):
    print(f"Page Number : {i+1}",end="\n")
    print(apple[i].page_content,end="\n")

apple[5].page_content

len(apple)

#data chunking
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    encoding_name='cl100k_base',
    chunk_size=512,
    chunk_overlap= 20
)

document_chunks = pdf_loader.load_and_split(text_splitter)

len(document_chunks)

document_chunks[0].page_content

document_chunks[-2].page_content

document_chunks[-1].page_content

#embeddings

!pip install --upgrade --force-reinstall sentence-transformers

from langchain_community.embeddings import SentenceTransformerEmbeddings
embedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')

embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)
embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)

print("Dimension of the embedding vector ",len(embedding_1))
len(embedding_1)==len(embedding_2)

#vector database

import os

out_dir = 'apple_db'

if not os.path.exists(out_dir):
  os.makedirs(out_dir)

vectorstore = Chroma.from_documents(
    document_chunks,
    embedding_model,
    persist_directory=out_dir
)

vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)

vectorstore.embeddings

vectorstore.similarity_search("Apple Steve Jobs iPhone ",k=3)

retriever = vectorstore.as_retriever(
    search_type='similarity',
    search_kwargs={'k': 2}
)

rel_docs = retriever.get_relevant_documents("How does does Apple develop and ship products that requires good coordination between the teams?")
rel_docs

model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
model_basename = "mistral-7b-instruct-v0.2.Q6_K.gguf"

from huggingface_hub import hf_hub_download

model_path = hf_hub_download(
    repo_id=model_name_or_path,
    filename=model_basename
)

pip install llama-cpp-python --force-reinstall --upgrade --prefer-binary

from llama_cpp import Llama
llm = Llama(model_path=model_path, n_gpu_layers=0)  # disables GPU

#uncomment the below snippet of code if the runtime is connected to CPU only.
from llama_cpp import Llama
llm = Llama(
model_path=model_path,
  n_ctx=1024,
   n_cores=-2
)

llm("How does does Apple develop and ship products that requires good coordination between the teams?")['choices'][0]['text']

qna_system_message = """
You are an assistant whose work is to review the report and provide the appropriate answers from the context.
User input will have the context required by you to answer user questions.
This context will begin with the token: ###Context.
The context contains references to specific portions of a document relevant to the user query.

User questions will begin with the token: ###Question.

Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.

If the answer is not found in the context, respond "I don't know".
"""

qna_user_message_template = """
###Context
Here are some documents that are relevant to the question mentioned below.
{context}

###Question
{question}
"""

def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):
    global qna_system_message,qna_user_message_template
    # Retrieve relevant document chunks
    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)
    context_list = [d.page_content for d in relevant_document_chunks]

    # Combine document chunks into a single context
    context_for_query = ". ".join(context_list)

    user_message = qna_user_message_template.replace('{context}', context_for_query)
    user_message = user_message.replace('{question}', user_input)

    prompt = qna_system_message + '\n' + user_message

    # Generate the response
    try:
        response = llm(
                  prompt=prompt,
                  max_tokens=max_tokens,
                  temperature=temperature,
                  top_p=top_p,
                  top_k=top_k
                  )

        # Extract and print the model's response
        response = response['choices'][0]['text'].strip()
    except Exception as e:
        response = f'Sorry, I encountered the following error: \n {e}'

    return response

user_input = "Who are the authors of this article and who published this article ?"
print(generate_rag_response(user_input))

user_input_2 = "List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
generate_rag_response(user_input_2)

user_input_3 = "Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
generate_rag_response(user_input_3)

user_input = "Who are the authors of this article and who published this article ?"
generate_rag_response(user_input, max_tokens=100)

user_input_2 = "List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
generate_rag_response(user_input_2, temperature=0.1, max_tokens=350)

user_input_3 = "Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
generate_rag_response(user_input_3, top_p=0.98, top_k=20, max_tokens=256)

groundedness_rater_system_message = """
You are tasked with rating AI generated answers to questions posed by users.
You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.
In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.

Evaluation criteria:
The task is to judge the extent to which the metric is followed by the answer.
1 - The metric is not followed at all
2 - The metric is followed only to a limited extent
3 - The metric is followed to a good extent
4 - The metric is followed mostly
5 - The metric is followed completely

Metric:
The answer should be derived only from the information presented in the context

Instructions:
1. First write down the steps that are needed to evaluate the answer as per the metric.
2. Give a step-by-step explanation if the answer adheres to the metric considering the question and context as the input.
3. Next, evaluate the extent to which the metric is followed.
4. Use the previous information to rate the answer using the evaluaton criteria and assign a score.
"""

relevance_rater_system_message = """
You are tasked with rating AI generated answers to questions posed by users.
You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.
In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.

Evaluation criteria:
The task is to judge the extent to which the metric is followed by the answer.
1 - The metric is not followed at all
2 - The metric is followed only to a limited extent
3 - The metric is followed to a good extent
4 - The metric is followed mostly
5 - The metric is followed completely

Metric:
Relevance measures how well the answer addresses the main aspects of the question, based on the context.
Consider whether all and only the important aspects are contained in the answer when evaluating relevance.

Instructions:
1. First write down the steps that are needed to evaluate the context as per the metric.
2. Give a step-by-step explanation if the context adheres to the metric considering the question as the input.
3. Next, evaluate the extent to which the metric is followed.
4. Use the previous information to rate the context using the evaluaton criteria and assign a score.
"""

user_message_template = """
###Question
{question}

###Context
{context}

###Answer
{answer}
"""

def generate_ground_relevance_response(user_input,k=1,max_tokens=128,temperature=0,top_p=0.95,top_k=50):
    global qna_system_message,qna_user_message_template
    # Retrieve relevant document chunks
    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)
    context_list = [d.page_content for d in relevant_document_chunks]
    context_for_query = ". ".join(context_list)

    # Combine user_prompt and system_message to create the prompt
    prompt = f"""[INST]{qna_system_message}\n
                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}
                [/INST]"""

    response = llm(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop=['INST'],
            )

    answer =  response["choices"][0]["text"]

    # Combine user_prompt and system_message to create the prompt
    groundedness_prompt = f"""[INST]{groundedness_rater_system_message}\n
                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}
                [/INST]"""

    # Combine user_prompt and system_message to create the prompt
    relevance_prompt = f"""[INST]{relevance_rater_system_message}\n
                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}
                [/INST]"""

    response_1 = llm(
            prompt=groundedness_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop=['INST'],
            )

    response_2 = llm(
            prompt=relevance_prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            stop=['INST'],
            )

    return response_1['choices'][0]['text'],response_2['choices'][0]['text']

user_input = "Who are the authors of this article and who published this article ?"
ground,rel = generate_ground_relevance_response(user_input,max_tokens=350)

print(ground,end="\n\n")
print(rel)

user_input_2 = "List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
ground,rel = generate_ground_relevance_response(user_input_2,max_tokens=500)

print(ground,end="\n\n")
print(rel)

user_input_3 = "Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
ground,rel = generate_ground_relevance_response(user_input_3,max_tokens=500)

print(ground,end="\n\n")
print(rel)

